{"cells":[{"cell_type":"markdown","id":"613c3a02-7d6d-46f8-984a-80a60f471500","metadata":{},"source":["# Introduction to Deep Learning with PyTorch"]},{"cell_type":"markdown","id":"ce04b23d-5ee1-4e1d-91c0-0420560bd84f","metadata":{},"source":["## Basic informations about tensors"]},{"cell_type":"code","execution_count":null,"id":"d0eb4f16-5a99-460d-a5ba-706b7ef0bbe7","metadata":{"executionTime":0,"id":"bA5ajAmk7XH6","lastSuccessfullyExecutedCode":"import torch\nimport numpy as np"},"outputs":[],"source":["import torch\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"id":"c438b727-78cb-45eb-882d-afdadd5d1c1c","metadata":{"executionTime":39,"lastSuccessfullyExecutedCode":"# create a tensor from a list\narray = [[1,2,3],[4,5,6]]\ntensor = torch.tensor(array)\ntensor"},"outputs":[],"source":["# create a tensor from a list\n","array = [[1,2,3],[4,5,6]]\n","tensor = torch.tensor(array)\n","tensor"]},{"cell_type":"code","execution_count":null,"id":"e35d577e-6e40-4adf-8247-49a852bb03ac","metadata":{"executionTime":47,"lastSuccessfullyExecutedCode":"# create a tensor from a NumPy arra\nnp_array = np.array(array)\nnp_tensor = torch.from_numpy(np_array)\nnp_tensor"},"outputs":[],"source":["# create a tensor from a NumPy arra\n","np_array = np.array(array)\n","np_tensor = torch.from_numpy(np_array)\n","np_tensor"]},{"cell_type":"code","execution_count":null,"id":"1630fc76-bf94-4f57-9f28-6ac7df29e7ce","metadata":{"executionTime":82,"lastSuccessfullyExecutedCode":"# tensor attributes\ntensor.device, tensor.shape, tensor.dtype"},"outputs":[],"source":["# tensor attributes\n","tensor.device, tensor.shape, tensor.dtype"]},{"cell_type":"code","execution_count":null,"id":"5f609947-3f7f-48f4-93e3-892f2d5c296c","metadata":{"executionTime":60,"lastSuccessfullyExecutedCode":"# tensor operations (The size of tensor a must match the size of tensor b)\na = torch.tensor([[1,1],[2,2]]) \nb = torch.tensor([[2,2],[3,3]])\n# addition/substraction/multiplication/divizion\na+b, a-b, a*b, a/b"},"outputs":[],"source":["# tensor operations (The size of tensor a must match the size of tensor b)\n","a = torch.tensor([[1,1],[2,2]]) \n","b = torch.tensor([[2,2],[3,3]])\n","# addition/substraction/multiplication/divizion\n","a+b, a-b, a*b, a/b"]},{"cell_type":"markdown","id":"ec986f11-cc00-4c4f-952d-025e84ce3231","metadata":{},"source":["## Roadmap to training a neural network"]},{"cell_type":"markdown","id":"bd694fce-efbd-4df4-bebe-8d23c1c93975","metadata":{},"source":["1. Create an architecture of the nerural network (see an example bellow) ![nn_architecture](nn_architecture.png)\n","2. Load data using PyTorch\n","3. Define the loss function (Measures difference between model predictions and true labels; Used to assess the accuracy of our model; Accuracy is determined by weights and biases, which are parameters learned during model training; They denote strength and direction of connections between individual neurons.)\n","4. Set up an optimizer (Optimizer will update network weights during training; SGD most used)\n","5. Define a training loop (Pass input data through network to gain initial predictions, i.e. run a forward pass; Compute loss using loss function; Run backpropagation to compute gradients, which determine how much each neuron contributed to overall error in final output; Update weights and biases using the gradients)\n","6. Test the trained network on a separate dataset to evaluate performance (Define metrics to test usefulness of our predictions on validation and test sets)\n"]},{"cell_type":"markdown","id":"a476a8fc-6004-4f37-8625-c4ead5c40939","metadata":{},"source":["### 1. Create a neural network"]},{"cell_type":"markdown","id":"a5d11d87-c258-44d4-bade-fe337675b49a","metadata":{},"source":["**PyTorch implementation of a neural network with three layers.**\n","\n","- The input layer has 8 neurons and is represented by nn.Linear(8, 4), which means that it takes an input with 8 features and outputs a tensor with 4 values.\n","- The first hidden layer is represented by nn.Sigmoid(), which applies the sigmoid activation function to the output of the input layer.\n","- The second hidden layer has 2 neurons and is represented by nn.Linear(4, 2), which takes the output of the first hidden layer as input and outputs a tensor with 2 values.\n","- The second hidden layer is followed by another nn.Sigmoid() activation function.\n","- The output layer has only 1 neuron and is represented by nn.Linear(2, 1), which takes the output of the second hidden layer as input and produces a scalar value as output.\n","- The final nn.Sigmoid() activation function is applied to the output of the output layer, which maps the output to a value between 0 and 1, making it suitable for binary classification tasks.\n","\n","The network bellow has three layers with sigmoid activation functions, which makes it suitable for classification problems where the output is a binary variable."]},{"cell_type":"code","execution_count":null,"id":"32cba4bc-8bf4-4d28-a87a-74655edac3a1","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"import torch.nn as nn\n\nmodel = nn.Sequential(nn.Linear(8, 4),\n                      nn.Sigmoid(),\n                      nn.Linear(4, 2),\n                      nn.Sigmoid(),\n                      nn.Linear(2, 1),\n                      nn.Sigmoid())\n\ninput_tensor = torch.Tensor([[1, 2, 3, 4, 5, 6, 7, 8]])\noutput = model(input_tensor)"},"outputs":[],"source":["import torch.nn as nn\n","\n","model = nn.Sequential(nn.Linear(8, 4),\n","                      nn.Sigmoid(),\n","                      nn.Linear(4, 2),\n","                      nn.Sigmoid(),\n","                      nn.Linear(2, 1),\n","                      nn.Sigmoid())\n","\n","input_tensor = torch.Tensor([[1, 2, 3, 4, 5, 6, 7, 8]])\n","output = model(input_tensor)"]},{"cell_type":"markdown","id":"d2ae294b-d936-41cd-80a0-3cf3c253dd4e","metadata":{},"source":["Note: A neural network with only one layer can be similar to logistic regression if the activation function used in the output layer is the sigmoid function. In this case, the neural network is essentially a logistic regression model with additional trainable parameters.\n","\n","Input and output layers dimensions are fixed (input layer depends on the number of features n_features; output layer depends on the number of categories n_classes). Increasing the number of hidden layers = increasing the number of parameters = increasing the model capacity."]},{"cell_type":"markdown","id":"d498e580-7473-4015-8a6c-b9c723cbbd45","metadata":{},"source":["### 2. Load data in PyTorch"]},{"cell_type":"code","execution_count":null,"id":"5c123d3b-3e64-4f3e-b3a5-7671591d29ee","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"# Import libraries\nfrom torch.utils.data import Dataset\nimport pandas as pd\nfrom torch.utils.data import DataLoader, TensorDataset"},"outputs":[],"source":["# Import libraries\n","from torch.utils.data import Dataset\n","import pandas as pd\n","from torch.utils.data import DataLoader, TensorDataset"]},{"cell_type":"code","execution_count":null,"id":"35144ff1-e794-4919-93b8-fcbbad969a0d","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"# Create a PyTorch Dataset class that reads a CSV file and provides a method for extracting features and labels for a given index.\n\nclass MyDataset(Dataset):\n    def __init__(self, csv_path):\n        \"\"\"\n        A PyTorch dataset for loading data from a CSV file.\n        \n        Parameters:\n            csv_path (str): The path to the CSV file.\n        \n        Returns:\n            None\n        \"\"\"\n        super(MyDataset, self).__init__()\n        self.data = pd.read_csv(csv_path).to_numpy()\n        \n    def __len__(self):\n        \"\"\"\n        Get the number of samples in the dataset.\n        \n        Parameters:\n            None\n        \n        Returns:\n            int: The number of samples in the dataset.\n        \"\"\"\n        return self.data.shape[0]\n    \n    def __getitem__(self, index):\n        \"\"\"\n        Get the features and label of a sample at a given index.\n        \n        Parameters:\n            index (int): The index of the sample to retrieve.\n        \n        Returns:\n            tuple: A tuple containing the features (as a tensor) and label (as a tensor) of the sample.\n        \"\"\"\n        features, label = self.extract_features_and_label(index)\n        return features, label\n    \n    def extract_features_and_label(self, index):\n        \"\"\"\n        Extract the features and label of a sample at a given index.\n        \n        Parameters:\n            index (int): The index of the sample to retrieve.\n        \n        Returns:\n            tuple: A tuple containing the features (as a tensor) and label (as a tensor) of the sample.\n        \"\"\"\n        row = self.data[index]\n        features = row[:-1].astype(np.float32)\n        label = torch.tensor(row[-1])\n        return torch.from_numpy(features), torch.tensor(label)"},"outputs":[],"source":["# Create a PyTorch Dataset class that reads a CSV file and provides a method for extracting features and labels for a given index.\n","\n","class MyDataset(Dataset):\n","    def __init__(self, csv_path):\n","        \"\"\"\n","        A PyTorch dataset for loading data from a CSV file.\n","        \n","        Parameters:\n","            csv_path (str): The path to the CSV file.\n","        \n","        Returns:\n","            None\n","        \"\"\"\n","        super(MyDataset, self).__init__()\n","        self.data = pd.read_csv(csv_path).to_numpy()\n","        \n","    def __len__(self):\n","        \"\"\"\n","        Get the number of samples in the dataset.\n","        \n","        Parameters:\n","            None\n","        \n","        Returns:\n","            int: The number of samples in the dataset.\n","        \"\"\"\n","        return self.data.shape[0]\n","    \n","    def __getitem__(self, index):\n","        \"\"\"\n","        Get the features and label of a sample at a given index.\n","        \n","        Parameters:\n","            index (int): The index of the sample to retrieve.\n","        \n","        Returns:\n","            tuple: A tuple containing the features (as a tensor) and label (as a tensor) of the sample.\n","        \"\"\"\n","        features, label = self.extract_features_and_label(index)\n","        return features, label\n","    \n","    def extract_features_and_label(self, index):\n","        \"\"\"\n","        Extract the features and label of a sample at a given index.\n","        \n","        Parameters:\n","            index (int): The index of the sample to retrieve.\n","        \n","        Returns:\n","            tuple: A tuple containing the features (as a tensor) and label (as a tensor) of the sample.\n","        \"\"\"\n","        row = self.data[index]\n","        features = row[:-1].astype(np.float32)\n","        label = torch.tensor(row[-1])\n","        return torch.from_numpy(features), torch.tensor(label)"]},{"cell_type":"code","execution_count":null,"id":"44b36241-6eff-462d-ae53-ee9d80824840","metadata":{"executionTime":45,"lastSuccessfullyExecutedCode":"# Create an instance of the MyDataset class and a DataLoader to load the data in batches.\ndataset = MyDataset('water_potability.csv')\ndataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n\n# Iterates over the DataLoader to extract features and labels in batches, and prints the shape of the features and labels in each batch. \nfor batch_idx, (features, label) in enumerate(dataloader):\n    print(f'Batch {batch_idx}: {features.shape}, {label.shape}')\n\n# Extract the first batch of features and labels using the next function.\nx, y = next(iter(dataloader))"},"outputs":[],"source":["# Create an instance of the MyDataset class and a DataLoader to load the data in batches.\n","dataset = MyDataset('water_potability.csv')\n","dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n","\n","# Iterates over the DataLoader to extract features and labels in batches, and prints the shape of the features and labels in each batch. \n","for batch_idx, (features, label) in enumerate(dataloader):\n","    print(f'Batch {batch_idx}: {features.shape}, {label.shape}')\n","\n","# Extract the first batch of features and labels using the next function.\n","x, y = next(iter(dataloader))"]},{"cell_type":"code","execution_count":null,"id":"301c252f-a57d-4bbf-8b4a-398e93346976","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"# Create a model using the nn.Sequential API\nmodel = nn.Sequential(nn.Linear(4, 16), nn.Linear(16, 1))"},"outputs":[],"source":["# Create a model using the nn.Sequential API\n","model = nn.Sequential(nn.Linear(4, 16), nn.Linear(16, 1))"]},{"cell_type":"markdown","id":"910eac02-bbe6-4537-b02b-0688ed683572","metadata":{},"source":["### 3. Define the loss function"]},{"cell_type":"markdown","id":"3e90f8c2-0a99-41d9-bea7-39f3ec37a2b7","metadata":{},"source":["The cross entropy loss is the most used loss for classification problems. "]},{"cell_type":"code","execution_count":null,"id":"e9ff70d7-2bce-43b5-bef5-45fda5b35e2b","metadata":{"executionTime":31,"lastSuccessfullyExecutedCode":"import torch.nn.functional as F\n\ny = [2]\nscores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n\n# Create a one-hot encoded vector of the label y\none_hot_label = F.one_hot(torch.tensor(y), scores.shape[1])\n\n# Create the cross entropy loss function\nloss = nn.CrossEntropyLoss()\n\n# Calculate the cross entropy loss\nloss(scores.double(), one_hot_label.double())"},"outputs":[],"source":["import torch.nn.functional as F\n","\n","y = [2]\n","scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n","\n","# Create a one-hot encoded vector of the label y\n","one_hot_label = F.one_hot(torch.tensor(y), scores.shape[1])\n","\n","# Create the cross entropy loss function\n","loss = nn.CrossEntropyLoss()\n","\n","# Calculate the cross entropy loss\n","loss(scores.double(), one_hot_label.double())"]},{"cell_type":"markdown","id":"8f3f6996-8ff2-47c4-b17f-e1c3572e0452","metadata":{},"source":["Note: \n","This code is calculating the cross entropy loss for a single sample with four classes.\n","First, it defines the ground truth label y as class 2. Then, it creates a tensor scores of shape (1, 4) with the predicted scores for each class.\n","Next, it uses the F.one_hot function to create a one-hot encoded tensor of y with the same number of classes as scores. If the ground truth label y is class 2, it means that the correct class for the given sample is the one represented by the third element (index 2) of the scores tensor. The resulting one_hot_label tensor is of shape (1, 4) and has a 1 in the 2nd position (corresponding to the ground truth class) and 0s elsewhere.\n","Finally, it creates an instance of the nn.CrossEntropyLoss() class and applies it to the scores tensor and the one_hot_label tensor using the .double() method to ensure that both tensors have the same data type (double precision).\n","The output of the loss function is a scalar tensor representing the cross entropy loss between the predicted scores and the one-hot encoded ground truth label."]},{"cell_type":"markdown","id":"04af44b9-82b0-41c2-ad39-cdffb39856ac","metadata":{},"source":["### 4. Set up an optimizer"]},{"cell_type":"markdown","id":"29cd77b4-51e2-45fc-8b0c-c2d5d00dfb3c","metadata":{},"source":["In PyTorch, an optimizer takes care of weight updates. The most common optimizer is stochastic gradient descent (SGD)."]},{"cell_type":"markdown","id":"9ac51d9a-c363-47c9-896e-4fcacffdeb42","metadata":{},"source":["### Manual optimization"]},{"cell_type":"code","execution_count":null,"id":"e4e1c4c6-5b35-47fe-b6be-9f8f57ab5d7a","metadata":{"executionTime":28,"lastSuccessfullyExecutedCode":"# Initialize the model and loss function\nmodel = nn.Sequential(nn.Linear(16, 8),\n                      nn.Linear(8, 4),\n                      nn.Linear(4, 2))\ncriterion = nn.CrossEntropyLoss()\n\n# Define the input data and target labels\ninputs = torch.randn(2, 16)\nlabels = torch.LongTensor([0, 1])\n\n# Compute the forward pass\noutputs = model(inputs)\n\n# Compute the loss\nloss = criterion(outputs, labels)\n\n# Compute the gradients using backpropagation\nloss.backward()\n\n# Learning rate is typically small\nlr = 0.001\n\n# Update the weights\nweight = model[0].weight\nweight_grad = model[0].weight.grad\nweight = weight - lr * weight_grad\n\n# Update the biases\nbias = model[0].bias\nbias_grad = model[0].bias.grad\nbias = bias - lr * bias_grad\n\nprint(weight, bias)"},"outputs":[],"source":["# Initialize the model and loss function\n","model = nn.Sequential(nn.Linear(16, 8),\n","                      nn.Linear(8, 4),\n","                      nn.Linear(4, 2))\n","criterion = nn.CrossEntropyLoss()\n","\n","# Define the input data and target labels\n","inputs = torch.randn(2, 16)\n","labels = torch.LongTensor([0, 1])\n","\n","# Compute the forward pass\n","outputs = model(inputs)\n","\n","# Compute the loss\n","loss = criterion(outputs, labels)\n","\n","# Compute the gradients using backpropagation\n","loss.backward()\n","\n","# Learning rate is typically small\n","lr = 0.001\n","\n","# Update the weights\n","weight = model[0].weight\n","weight_grad = model[0].weight.grad\n","weight = weight - lr * weight_grad\n","\n","# Update the biases\n","bias = model[0].bias\n","bias_grad = model[0].bias.grad\n","bias = bias - lr * bias_grad\n","\n","print(weight, bias)"]},{"cell_type":"markdown","id":"cc2006ea-c3c0-4666-a7f2-5c43d302a8d0","metadata":{},"source":["### Using the PyTorch optimize"]},{"cell_type":"code","execution_count":null,"id":"a5071d23-2153-4f65-a5c7-4ab9f4bd3548","metadata":{"executionTime":34,"lastSuccessfullyExecutedCode":"import torch.optim as optim\n\n# Create the optimizer\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\n# Update the model's parameters using the optimizer\noptimizer.step()\n\nprint(weight, bias)"},"outputs":[],"source":["import torch.optim as optim\n","\n","# Create the optimizer\n","optimizer = optim.SGD(model.parameters(), lr=0.001)\n","\n","# Update the model's parameters using the optimizer\n","optimizer.step()\n","\n","print(weight, bias)"]},{"cell_type":"markdown","id":"b6e6fefd-971b-41ce-8a10-f912689fe662","metadata":{},"source":["Note: In the code above, the optimizer is an instance of the stochastic gradient descent (SGD) algorithm, which is a popular optimization algorithm used in deep learning. The learning rate (lr) is set to 0.001, which determines how much the optimizer should adjust the parameters based on the gradients of the loss function.\n","After defining the optimizer, the step() method is called to update the model's parameters based on the gradients computed during backpropagation. This step is typically executed inside a training loop, where the model is iteratively trained on mini-batches of data."]},{"cell_type":"markdown","id":"b72636c6-9dd5-43f5-bb5f-d0c2f6068f93","metadata":{},"source":["### 5. Define a training loop "]},{"cell_type":"code","execution_count":null,"id":"f74dd190-18de-4b4a-9ad9-944911bb1c02","metadata":{"executionTime":135,"lastSuccessfullyExecutedCode":"# Create an instance of the MyDataset class and a DataLoader to load the data in batches.\ndataset = MyDataset('ds_salaries_clean.csv')\ndataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n\n# Create the model\nmodel = nn.Sequential(nn.Linear(4, 2),nn.Sigmoid(),nn.Linear(2, 1))\n\n# Create the loss and optimizer\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\nnum_epochs = 10\n\n# Loop through the dataset multiple times\nfor epoch in range(num_epochs):\n    for data in dataloader:\n        # Set the gradients to zero\n        optimizer.zero_grad()\n        # Get feature and target from the data loader\n        feature, target = data\n        # Run a forward pass\n        pred = model(feature)\n        # Compute loss and gradients\n        loss = criterion(pred, target)\n        loss.backward()\n        # Update the parameters\n        optimizer.step()\n\ndef show_results(model, dataloader):\n    \"\"\"\n    The show_results function takes a PyTorch model and a PyTorch DataLoader object as input and    prints the ground truth and predicted values for each batch in the DataLoader.\n\n    Parameters:\n            model: a PyTorch model object that takes in features as input and outputs predicted                     values.\n            dataloader: a PyTorch DataLoader object that contains the input features and target                     salaries in batches.\n    Returns:\n            None.\n    \"\"\"\n    with torch.no_grad():\n        for data in dataloader:\n            feature, target = data\n            pred = model(feature)\n            for i in range(len(feature)):\n                print(f\"Truth value: {target[i]:.3f}. Predicted value: {pred[i][0]:.3f}.\")\n                \nshow_results(model, dataloader)"},"outputs":[],"source":["# Create an instance of the MyDataset class and a DataLoader to load the data in batches.\n","dataset = MyDataset('data/ds_salaries_clean.csv')\n","dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n","\n","# Create the model\n","model = nn.Sequential(nn.Linear(4, 2),nn.Sigmoid(),nn.Linear(2, 1))\n","\n","# Create the loss and optimizer\n","criterion = nn.MSELoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001)\n","\n","num_epochs = 10\n","\n","# Loop through the dataset multiple times\n","for epoch in range(num_epochs):\n","    for data in dataloader:\n","        # Set the gradients to zero\n","        optimizer.zero_grad()\n","        # Get feature and target from the data loader\n","        feature, target = data\n","        # Run a forward pass\n","        pred = model(feature)\n","        # Compute loss and gradients\n","        loss = criterion(pred, target)\n","        loss.backward()\n","        # Update the parameters\n","        optimizer.step()\n","\n","def show_results(model, dataloader):\n","    \"\"\"\n","    The show_results function takes a PyTorch model and a PyTorch DataLoader object as input and    prints the ground truth and predicted values for each batch in the DataLoader.\n","\n","    Parameters:\n","            model: a PyTorch model object that takes in features as input and outputs predicted                     values.\n","            dataloader: a PyTorch DataLoader object that contains the input features and target                     salaries in batches.\n","    Returns:\n","            None.\n","    \"\"\"\n","    with torch.no_grad():\n","        for data in dataloader:\n","            feature, target = data\n","            pred = model(feature)\n","            for i in range(len(feature)):\n","                print(f\"Truth value: {target[i]:.3f}. Predicted value: {pred[i][0]:.3f}.\")\n","                \n","show_results(model, dataloader)"]},{"cell_type":"markdown","id":"262bd2e4-6a0c-417a-b6ab-787bbd8294b6","metadata":{},"source":["Note: The code above trains a neural network model using PyTorch to predict salaries based on a dataset of employee features. The dataset is loaded into an instance of the MyDataset class and a DataLoader is created to load the data in batches. The neural network model consists of two linear layers with sigmoid activation function between them. The mean squared error loss function and stochastic gradient descent optimizer are used to train the model for 10 epochs. The show_results function is then called to display the predicted and ground truth salaries for each data point in the dataset."]},{"cell_type":"markdown","id":"c8af21d3-51c8-4f21-998a-87621d4dbc0c","metadata":{},"source":["### 6. Test the trained network on a separate dataset to evaluate performance"]},{"cell_type":"code","execution_count":null,"id":"0b3d0f6d-52fa-4549-bbe9-f5d62e9ba53e","metadata":{"executionTime":117,"lastSuccessfullyExecutedCode":"# Set the model to evaluation mode\nmodel.eval()\nvalidation_loss = 0.0\nwith torch.no_grad():\n  for data in validationloader:\n      outputs = model(data[0])\n      loss = criterion(outputs, data[1])\n      # Sum the current loss to the validation_loss variable\n      validation_loss += loss.item()\n# Calculate the mean loss value\nvalidation_loss_epoch = validation_loss / len(validationloader)\n# Set the model back to training mode\nmodel.train()"},"outputs":[],"source":["# Set the model to evaluation mode\n","model.eval()\n","validation_loss = 0.0\n","with torch.no_grad():\n","  for data in validationloader:\n","      outputs = model(data[0])\n","      loss = criterion(outputs, data[1])\n","      # Sum the current loss to the validation_loss variable\n","      validation_loss += loss.item()\n","# Calculate the mean loss value\n","validation_loss_epoch = validation_loss / len(validationloader)\n","# Set the model back to training mode\n","model.train()"]},{"cell_type":"code","execution_count":null,"id":"216dcbf3-043f-4216-a4c1-c1b231804360","metadata":{"executionCancelledAt":1679729297320},"outputs":[],"source":["import torchmetrics\n","\n","# Create accuracy metric using torch metrics\n","metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3)\n","for data in dataloader:\n","    features, labels = data\n","    outputs = model(features)\n","    \n","    # Calculate accuracy over the batch\n","    acc = metric(outputs.softmax(dim=-1), labels.argmax(dim=-1))\n","    \n","# Calculate accuracy over the whole epoch\n","acc = metric.compute()\n","\n","# Reset the metric for the next epoch \n","metric.reset()"]},{"cell_type":"markdown","id":"3e2e1463-99f5-4726-8ed1-4a4a54ab2b4f","metadata":{},"source":["### Improve performance"]},{"cell_type":"markdown","id":"e429fb3d-6f7a-4b4a-95e6-aa52ac555405","metadata":{},"source":["Steps to maximize performance: overfit the training set, reduce overfitting, fine-tune the hyperparamters"]},{"cell_type":"code","execution_count":null,"id":"b7b9fa24-4115-4941-ae14-221accab6335","metadata":{"executionCancelledAt":1679729297353},"outputs":[],"source":["# overfit the training set\n","\n","features, labels = next(iter(trainloader))\n","for i in range(1e3):\n","    outputs = model(features)\n","    loss = criterion(outputs, labels)\n","    loss.backward()\n","    optimizer.step()"]},{"cell_type":"code","execution_count":null,"id":"d27c73e6-a6e7-4cea-b3b6-784ac7ae84c6","metadata":{"executionCancelledAt":1679729297421},"outputs":[],"source":["# reduce overfitting (the model does not generalize to unseen data)"]},{"cell_type":"code","execution_count":null,"id":"7677692c-f4be-4d70-942d-dfa2cac69a21","metadata":{"executionCancelledAt":1679729297468},"outputs":[],"source":["# 1. If the dataset in not large enought, get more data or use data augmentation\n","\n","from torchvision import transforms\n","\n","# Create a data augmentation strategy using at least one transform\n","augmentation = nn.Sequential(transforms.RandomHorizontalFlip(p=0.5))\n","\n","# Create a data augmentation strategy using all three transforms.\n","augmentation = nn.Sequential(transforms.RandomHorizontalFlip(p=0.5),\n","                             transforms.RandomResizedCrop(size=32, scale=(0.3, 1.5)),\n","                             transforms.RandomRotation(degrees=20))"]},{"cell_type":"code","execution_count":null,"id":"76862495-af3e-44f7-bae4-cbaba2a61d1c","metadata":{"executionCancelledAt":1679729297510},"outputs":[],"source":["# 2. If the model has too much capacity, reduce model size or add dropout\n","model = nn.Sequential(nn.Linear(8, 4), nn.ReLU(), nn.Dropout(p=0.5))\n","features = torch.randn((1, 8))\n","#model(i)\n","\n","#Behaves differently during training and evaluation. Do not forget to switch modes using model.train() and model.eval()"]},{"cell_type":"code","execution_count":null,"id":"dde10ada-9dd4-4782-8473-5d9b864abe3c","metadata":{"executionCancelledAt":1679729297530},"outputs":[],"source":["# 3. If weights are too large, weight decay\n","optimizer = optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-4)\n","\n","#weight_decay parameter:values between 0 and 1. Typically, a very small value (10^-4 ). the higher the parameter, the less likely the model is to overfit."]},{"cell_type":"code","execution_count":null,"id":"66db84b9-6ab5-4c9c-9a4c-6818d552f1e4","metadata":{"executionCancelledAt":1679729297571},"outputs":[],"source":["# fine-tune hyperparameters"]},{"cell_type":"code","execution_count":null,"id":"64c32eaa-ff10-4fa2-8826-dc64e13b8231","metadata":{"executionCancelledAt":1679729297602},"outputs":[],"source":["# implement random search\n","\n","values = []\n","for idx in range(10):\n","    # Randomly sample a learning rate factor between 0.01 and 0.0001\n","    factor = np.random.uniform(2, 6)\n","    lr = 10 ** -factor\n","    \n","    # Randomly sample a momentum between 0.85 and 0.99\n","    momentum = np.random.uniform(0.85, 0.99)\n","    \n","    values.append((lr, momentum))"]},{"cell_type":"code","execution_count":null,"id":"c83f2e38-c17f-4fd0-91e4-6567fe3f9999","metadata":{"executionCancelledAt":1679729297647},"outputs":[],"source":["# create the best model  \n","model = nn.Sequential(transforms.Normalize(mean, std),\n","                      transforms.RandomHorizontalFlip(p=0.5),\n","                      transforms.RandomResizedCrop(size=32, scale=(0.8, 1.2)),\n","                      transforms.RandomRotation(degrees=10),\n","                      nn.Flatten(),\n","                      nn.Linear(3072, 3))\n","\n","# Pick a learning rate\n","lr = 0.01\n","train_and_evaluate(model, learning_rate=lr)\n","\n","# Adjust other parameters\n","train_and_evaluate(model, learning_rate=0.01, num_epochs=20, momentum=0.9, weight_decay=1e-4)"]},{"cell_type":"markdown","id":"35027f9a-dec2-4b82-ac28-cd834c5c45e8","metadata":{},"source":["# More about NN"]},{"cell_type":"markdown","id":"35f72d4c-acd3-4152-bf12-02c96572992c","metadata":{},"source":["### Activation functions"]},{"cell_type":"code","execution_count":null,"id":"d697a262-ae63-4684-8bd2-08edb5aefe25","metadata":{"executionCancelledAt":1679729297676},"outputs":[],"source":["# Implement ReLU in NumPy\n","def relu_numpy(x):\n","  # Implement the ReLU function\n","  return np.maximum(x,0)\n","\n","# Create a ReLU function with PyTorch\n","relu_pytorch = nn.ReLU()\n","\n","# Calculate the gradient of the ReLU function for x\n","x = torch.tensor(-1.0, requires_grad=True)\n","y = relu_pytorch(x)\n","y.backward()\n","gradient = x.grad\n","\n","# Implementing leaky ReLU\n","def leaky_relu_python(x, slope):\n","  # Implement the leaky_relu function\n","  if x >=0:\n","    return x\n","  else:\n","    return x * slope\n","\n","# Create a leaky relu function in PyTorch\n","leaky_relu_pytorch = nn.LeakyReLU(negative_slope = 0.05)\n","\n","y = torch.tensor(-2.0)\n","# Call the above function of the tensor y\n","output = leaky_relu_pytorch(y)\n"]},{"cell_type":"markdown","id":"5532a42c-c8a2-46ac-8d4b-2c16c6f57a85","metadata":{},"source":["### Model capacity"]},{"cell_type":"code","execution_count":null,"id":"14fb138f-69f1-44ba-8aa7-4a8c2df1c3af","metadata":{"executionCancelledAt":1679729297710},"outputs":[],"source":["def calculate_capacity(model):\n","  total = 0\n","  for p in model.parameters():\n","    total += p.numel()\n","  return total\n","\n","n_features = 8\n","n_classes = 2\n","\n","# Create a neural network with less than 120 parameters\n","model = nn.Sequential(nn.Linear(n_features, 8),\n","                      nn.Linear(8, 4),\n","                      nn.Linear(4, n_classes))\n","print(calculate_capacity(model))\n","\n","# Create a neural network with more than 120 parameters\n","model = nn.Sequential(nn.Linear(n_features, 8),\n","                      nn.Linear(8, 6),\n","                      nn.Linear(6, n_classes))\n","calculate_capacity(model)"]},{"cell_type":"markdown","id":"b194bbe0-5b94-44f6-8087-810ac9f3e75b","metadata":{},"source":["### Learning rate and momentum"]},{"cell_type":"markdown","id":"daa47c10-42e7-4a3e-b124-78c5fedc53f3","metadata":{},"source":["SGD has two parameters: learning rate that controls the step size and momentul that controls the inertia of the optimizer. Bad values can lead to long raining time and bad overall performance (poor accuracy).\n","\n","- Learning rate: controls the step size; too small leads to long training times; too high leads to poor performance; typical values between 10^-2 and 10^-4.\n","- Momentum: controls the inertia; null momentum can lead to the optimizer being stuck in a local minimum; non-null momentum can help find the function minimum; typical values between 0.85 and 0.99."]},{"cell_type":"markdown","id":"f3d024dd-8e60-4d7d-8052-4c89a3ff70b2","metadata":{},"source":["### Layer initialization, transfer learning and fine-tuning"]},{"cell_type":"markdown","id":"82829d67-7a00-47ca-83f0-5765a332a812","metadata":{},"source":["Fine-tuning process: find a model trained on a similar task; load pre-trained weights; freeze (or not) some of the layers in the model; train with a smaller lerning rate; look at the loss values and see if the learning rate needs to be adjusted"]},{"cell_type":"code","execution_count":null,"id":"8bf64b36-0c01-46f9-b86c-1e7bf52c3de2","metadata":{"executionCancelledAt":1679729297729},"outputs":[],"source":["# Layer initialization (weights are initialized to small values)\n","layer = nn.Linear(64, 128)\n","nn.init.uniform_(layer.weight)\n","print(layer.weight.min(), layer.weight.max())\n","\n","# Transfer learning: reusing a model trained on a first task for a second similar task, to accelerate the training process.\n","torch.save(layer, 'layer.pth')\n","new_layer = torch.load('layer.pth')\n","\n","# Fine-tuning (A type of transfer learning; Smaller learning rate; Not every layer is trained (we freeze some of them); Rule of thumb: freeze early layers of network and fine-tune layers closer to output layer)\n","model = nn.Sequential(nn.Linear(64, 128), nn.Linear(128, 256))\n","for name, param in model.named_parameters():\n","    # Check if the parameters belong to the first layer\n","    if name == '0.weight':\n","        # Freeze the parameters\n","        param.requires_grad = False"]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}
